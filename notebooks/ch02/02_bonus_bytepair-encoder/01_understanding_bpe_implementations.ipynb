{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fc85ae0",
   "metadata": {},
   "source": [
    "# My Learning Journey: Understanding Byte Pair Encoding (BPE)\n",
    "\n",
    "**Date**: January 27, 2026  \n",
    "**Learning Goal**: I'm deepening my understanding of how BPE tokenization works by exploring different implementations\n",
    "\n",
    "## What I'm Learning Today\n",
    "\n",
    "I'm revisiting Byte Pair Encoding (BPE), which is the tokenization method used in GPT-2 and many other LLMs. I want to understand:\n",
    "\n",
    "1. **Why BPE exists** - What problem does it solve compared to simple word splitting?\n",
    "2. **How different libraries implement BPE** - tiktoken, transformers, and the original OpenAI code\n",
    "3. **Performance differences** - Which implementation is fastest and why?\n",
    "4. **Practical usage** - How do I actually use these tokenizers in my code?\n",
    "\n",
    "## My Understanding So Far\n",
    "\n",
    "From Chapter 2, I learned that BPE:\n",
    "- Starts with individual characters as base tokens\n",
    "- Iteratively merges the most frequent pair of adjacent tokens\n",
    "- Builds up a vocabulary of subword units\n",
    "- Balances between character-level (flexible but long sequences) and word-level (fixed vocab, OOV issues)\n",
    "\n",
    "**I still need to clarify**: How exactly does the merging algorithm decide which pairs to merge? And how does the pre-tokenization step (splitting on whitespace/punctuation) interact with BPE?\n",
    "\n",
    "## Source Attribution\n",
    "\n",
    "This learning notebook synthesizes concepts from:\n",
    "- **Book**: *Build a Large Language Model From Scratch* by Sebastian Raschka  \n",
    "- **Reference**: Author's BPE comparison notebook in `source-material/ch02_bonus_bpe_from_author/`\n",
    "- **Original BPE**: OpenAI's GPT-2 implementation (Modified MIT License)\n",
    "\n",
    "I'm writing this in my own words to build deeper understanding, not just copying code.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881ca6a3",
   "metadata": {},
   "source": [
    "## Setup: Installing Required Packages\n",
    "\n",
    "I need some extra packages for this exploration. The main ones are:\n",
    "- **tiktoken** - OpenAI's fast BPE tokenizer (already in my environment)\n",
    "- **transformers** - Hugging Face library with GPT-2 tokenizer\n",
    "- **requests & tqdm** - For downloading vocabulary files\n",
    "\n",
    "**Note to self**: I'll keep these as optional dependencies since they're for comparison experiments, not core learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2a33e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what I already have installed\n",
    "from importlib.metadata import version\n",
    "import sys\n",
    "\n",
    "packages = ['tiktoken', 'torch', 'numpy']\n",
    "for package in packages:\n",
    "    try:\n",
    "        print(f\"{package}: {version(package)}\")\n",
    "    except Exception:\n",
    "        print(f\"{package}: NOT INSTALLED\")\n",
    "\n",
    "print(f\"\\nPython: {sys.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028abb82",
   "metadata": {},
   "source": [
    "**My observation**: tiktoken should already be available since it's in my pyproject.toml dependencies. If I want to run the transformers comparisons, I'll need to install that separately.\n",
    "\n",
    "Installing transformers (optional - only if I want to compare):\n",
    "```python\n",
    "# Uncomment to install\n",
    "# !pip install transformers requests tqdm\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48b7067",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Understanding tiktoken (OpenAI's Modern BPE)\n",
    "\n",
    "I'm starting with tiktoken because:\n",
    "1. It's the current recommended tokenizer from OpenAI\n",
    "2. It's significantly faster than the original GPT-2 encoder\n",
    "3. It's easier to use (cleaner API)\n",
    "\n",
    "**My mental model**: tiktoken takes text ‚Üí splits on regex patterns ‚Üí applies BPE merges ‚Üí returns token IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00aff6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "# I'm loading the GPT-2 BPE encoder\n",
    "# This uses the same vocabulary and merge rules as GPT-2\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# Let me see what this tokenizer does with a simple example\n",
    "text = \"Hello, world. Is this-- a test?\"\n",
    "print(f\"Original text: {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2772d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding: text ‚Üí token IDs\n",
    "token_ids = tokenizer.encode(text)\n",
    "print(f\"\\nToken IDs: {token_ids}\")\n",
    "print(f\"Number of tokens: {len(token_ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0d8e63",
   "metadata": {},
   "source": [
    "**I'm noticing**: The text got split into 9 tokens. Let me decode each one to see how BPE grouped the characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d9dd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let me decode each token individually to understand the subwords\n",
    "print(\"\\nBreakdown of each token:\")\n",
    "for i, token_id in enumerate(token_ids):\n",
    "    decoded = tokenizer.decode([token_id])\n",
    "    print(f\"  Token {i}: ID={token_id:5d} ‚Üí '{decoded}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011f5932",
   "metadata": {},
   "source": [
    "**My observations**:\n",
    "- \"Hello\" stayed as one token (common word)\n",
    "- \",\" and \".\" are separate tokens (punctuation gets its own tokens)\n",
    "- \" world\" includes the leading space (BPE encodes space as part of tokens)\n",
    "- \"--\" gets tokenized as one unit (punctuation sequences)\n",
    "- \" Is\", \" this\", \" a\", \" test\", \"?\" - notice the spaces are part of the tokens\n",
    "\n",
    "**Key insight I'm building**: BPE doesn't just split on spaces. It learned during training which character sequences appear frequently together, so \"Hello\" is one token because it's common in English text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d88551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can I go back from tokens to text?\n",
    "decoded_text = tokenizer.decode(token_ids)\n",
    "print(f\"\\nDecoded back: {decoded_text}\")\n",
    "print(f\"Same as original? {decoded_text == text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a36884",
   "metadata": {},
   "source": [
    "**Great!** The encoding is reversible (lossless). This is important for LLMs because they need to decode their predictions back into human-readable text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51495afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How big is the vocabulary?\n",
    "vocab_size = tokenizer.n_vocab\n",
    "print(f\"\\nVocabulary size: {vocab_size:,} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c781fb3",
   "metadata": {},
   "source": [
    "**My understanding**: GPT-2 uses a vocabulary of 50,257 tokens. This is much smaller than the ~100k words in English, but bigger than the 26 letters + punctuation. That's the sweet spot of BPE - balancing vocab size with sequence length.\n",
    "\n",
    "**I'm wondering**: What happens if I give it text with characters not in the vocabulary? Let me test with some emojis..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c04119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing with special characters\n",
    "emoji_text = \"Hello! üëã How are you doing? üòä\"\n",
    "emoji_tokens = tokenizer.encode(emoji_text)\n",
    "print(f\"Text with emojis: {emoji_text}\")\n",
    "print(f\"Number of tokens: {len(emoji_tokens)}\")\n",
    "print(f\"\\nToken breakdown:\")\n",
    "for i, tid in enumerate(emoji_tokens):\n",
    "    print(f\"  {i}: {tid:5d} ‚Üí '{tokenizer.decode([tid])}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2522211d",
   "metadata": {},
   "source": [
    "**My observation**: The emojis got broken down into multiple tokens! This makes sense - emojis are UTF-8 characters that get encoded as multiple bytes, and BPE operates on these bytes.\n",
    "\n",
    "**Takeaway**: BPE can handle ANY text (even emojis, Chinese characters, etc.) because it ultimately works on byte level. But rare characters will take up more tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6e2eea",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Comparing Different BPE Implementations\n",
    "\n",
    "I've learned that there are several implementations of BPE for GPT-2. I want to understand:\n",
    "1. Do they produce the same token IDs?\n",
    "2. Which one is faster?\n",
    "3. When should I use each one?\n",
    "\n",
    "### The implementations I'm comparing:\n",
    "1. **tiktoken** - Modern, fast, OpenAI's current recommendation\n",
    "2. **Original GPT-2 encoder** - Historical reference, slower but educational\n",
    "3. **Hugging Face transformers** - Widely used in the community\n",
    "\n",
    "**Note**: Since I don't have transformers installed yet, I'll focus on understanding the concepts. I can install it later if needed for actual experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83e9ce3",
   "metadata": {},
   "source": [
    "### Understanding the Original OpenAI Implementation\n",
    "\n",
    "The original GPT-2 encoder (from 2019) is in `source-material/ch02_bonus_bpe_from_author/bpe_openai_gpt2.py`. \n",
    "\n",
    "**What I learned from reading that code**:\n",
    "1. It uses a `bytes_to_unicode()` function to map UTF-8 bytes to printable Unicode characters\n",
    "2. The BPE algorithm iteratively merges character pairs based on learned merge rules\n",
    "3. It caches results to avoid recomputing the same tokens\n",
    "4. The pre-tokenization uses regex to split on words and punctuation\n",
    "\n",
    "**Key difference from tiktoken**: The original implementation is pure Python with lots of dictionary lookups. tiktoken is written in Rust for speed.\n",
    "\n",
    "**I'm not copying that code here** - instead, I'm building my understanding of the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe1dbb3",
   "metadata": {},
   "source": [
    "### My Mental Model of the BPE Algorithm\n",
    "\n",
    "Here's how I understand BPE works (in my own words):\n",
    "\n",
    "1. **Pre-tokenization**: Split text on whitespace and punctuation using regex\n",
    "   - Example: \"Hello, world!\" ‚Üí [\"Hello\", \",\", \" world\", \"!\"]\n",
    "\n",
    "2. **Byte encoding**: Convert each substring to bytes, then to a special Unicode representation\n",
    "   - This ensures we can handle any character set\n",
    "\n",
    "3. **BPE merging**: For each substring:\n",
    "   - Start with individual characters as tokens\n",
    "   - Look up which pairs can be merged (from pre-learned merge rules)\n",
    "   - Repeatedly merge the highest-priority pair until no more merges possible\n",
    "   - This creates subword tokens like \"Hello\", \"ing\", \"ed\", etc.\n",
    "\n",
    "4. **Token ID lookup**: Convert each subword token to its ID from the vocabulary\n",
    "\n",
    "**What I still find tricky**: Understanding exactly how the merge rules were originally learned (that involves counting pair frequencies in training data, which is a separate process)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa3333b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Performance Considerations\n",
    "\n",
    "From the author's experiments (see source-material), the performance ranking is roughly:\n",
    "\n",
    "1. **tiktoken** - Fastest (10-20x faster than original)\n",
    "   - Written in Rust with optimized string operations\n",
    "   - Minimal Python overhead\n",
    "   \n",
    "2. **Transformers (Fast)** - Fast (uses Rust tokenizers library)\n",
    "   - ~2-3x faster than original\n",
    "   - Slight overhead from the framework\n",
    "   \n",
    "3. **Original GPT-2** - Baseline (pure Python)\n",
    "   - Good for understanding the algorithm\n",
    "   - Too slow for production use\n",
    "   \n",
    "4. **Transformers (Python)** - Slower than original\n",
    "   - More features but slower due to framework overhead\n",
    "\n",
    "**My takeaway for practical use**: Use tiktoken for anything performance-critical. Use transformers if I need compatibility with other Hugging Face models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faaf93ae",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: When to Use Each Implementation\n",
    "\n",
    "Here's my decision tree for choosing a tokenizer:\n",
    "\n",
    "### Use **tiktoken** when:\n",
    "- ‚úÖ I'm working with OpenAI models (GPT-2, GPT-3, GPT-4)\n",
    "- ‚úÖ Performance matters (processing large datasets)\n",
    "- ‚úÖ I want clean, simple code\n",
    "- ‚úÖ I'm learning from scratch (like I am now!)\n",
    "\n",
    "### Use **Hugging Face transformers** when:\n",
    "- ‚úÖ I need compatibility with many different models\n",
    "- ‚úÖ I'm using pre-trained models from HuggingFace Hub\n",
    "- ‚úÖ I need additional features (padding, truncation, attention masks)\n",
    "- ‚úÖ I'm building end-to-end pipelines with the transformers library\n",
    "\n",
    "### Use **original encoder** when:\n",
    "- ‚úÖ I'm studying how BPE actually works (educational purposes)\n",
    "- ‚úÖ I need to understand legacy code\n",
    "- ‚ùå NOT for production (too slow)\n",
    "\n",
    "**My choice for this learning repo**: tiktoken, because it's fast, clean, and I'm focused on understanding fundamentals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab9cc5b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## My Takeaways and Next Steps\n",
    "\n",
    "### What I learned today:\n",
    "1. ‚úÖ BPE is a compression algorithm that balances vocabulary size and sequence length\n",
    "2. ‚úÖ tiktoken is the modern, fast way to use GPT-2's BPE encoding\n",
    "3. ‚úÖ BPE works on bytes, so it can handle any text (even emojis!)\n",
    "4. ‚úÖ Different implementations exist with different speed/feature tradeoffs\n",
    "\n",
    "### What I still need to practice:\n",
    "- üîÑ Implementing a simple BPE algorithm from scratch (to really understand the merge process)\n",
    "- üîÑ Understanding how the merge rules are learned from training data\n",
    "- üîÑ Experimenting with creating my own vocabulary on a small dataset\n",
    "- üîÑ Comparing how different vocabularies affect model performance\n",
    "\n",
    "### Next learning steps:\n",
    "1. Build a minimal BPE tokenizer from scratch (Chapter 2.6)\n",
    "2. Train it on a small text corpus to see how merge rules are learned\n",
    "3. Compare my implementation with tiktoken to validate correctness\n",
    "\n",
    "### References:\n",
    "- **Book**: *Build a Large Language Model From Scratch* by Sebastian Raschka, Chapter 2\n",
    "- **Source material**: `source-material/ch02_bonus_bpe_from_author/` (author's comparison)\n",
    "- **OpenAI**: Original GPT-2 encoder.py (Modified MIT License)\n",
    "- **tiktoken docs**: https://github.com/openai/tiktoken\n",
    "\n",
    "---\n",
    "\n",
    "**Date completed**: January 27, 2026  \n",
    "**Status**: Ready to move on to implementing BPE from scratch! üöÄ"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
