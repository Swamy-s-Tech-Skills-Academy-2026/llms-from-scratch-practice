{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c2b409d3",
      "metadata": {},
      "source": [
        "# Chapter 2: Working with Text Data\n",
        "\n",
        "- Focus: prepare and clean text for language modeling\n",
        "- Learn tokenization and build a small vocabulary\n",
        "- Turn raw text into numeric inputs we can train on"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f374a3f",
      "metadata": {},
      "source": [
        "Packages used in this notebook:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e68bf503",
      "metadata": {},
      "outputs": [],
      "source": [
        "from importlib.metadata import version\n",
        "\n",
        "print(\"torch version:\", version(\"torch\"))\n",
        "print(\"tiktoken version:\", version(\"tiktoken\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85d3ff76",
      "metadata": {},
      "source": [
        "## 2.1 Understanding word embeddings\n",
        "\n",
        "- For LLMs, we treat tokens as vectors in a high-dimensional space\n",
        "- These vectors encode relationships that the model can learn\n",
        "- We often use 2D sketches to build intuition about a much larger space"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28079f01",
      "metadata": {},
      "source": [
        "## 2.2 Tokenizing text\n",
        "\n",
        "- Tokenization splits text into units (words, punctuation, or subwords)\n",
        "- These units become the inputs the model will work with\n",
        "- I’ll use a public-domain text (The Verdict) as sample data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4dc50a0d",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Check for local file first (zero-copy policy compliance)\n",
        "# The file should already exist in the notebook directory\n",
        "file_path = \"the-verdict.txt\"\n",
        "\n",
        "if not os.path.exists(file_path):\n",
        "    # Fallback: download from public domain source if local file missing\n",
        "    # Note: This is a public domain text by Edith Wharton\n",
        "    import requests\n",
        "    url = (\n",
        "        \"https://raw.githubusercontent.com/rasbt/\"\n",
        "        \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
        "        \"the-verdict.txt\"\n",
        "    )\n",
        "    response = requests.get(url, timeout=30)\n",
        "    response.raise_for_status()\n",
        "    with open(file_path, \"wb\") as f:\n",
        "        f.write(response.content)\n",
        "    print(f\"Downloaded {file_path} from external source\")\n",
        "else:\n",
        "    print(f\"Using local file: {file_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8027639",
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "    \n",
        "print(\"Total number of character:\", len(raw_text))\n",
        "print(raw_text[:99])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "149d1ab5",
      "metadata": {},
      "source": [
        "- Goal: turn the raw text into tokens a model can consume\n",
        "- I’ll build a simple regex tokenizer first, then scale it to the full text\n",
        "- Start by splitting on whitespace to see the baseline behavior"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2679e062",
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "text = \"Hello, world. This, is a test.\"\n",
        "result = re.split(r'(\\s)', text)\n",
        "\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a3e334c",
      "metadata": {},
      "source": [
        "- Whitespace-only splitting misses punctuation boundaries\n",
        "- Next step: treat commas and periods as their own tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7eec2c50",
      "metadata": {},
      "outputs": [],
      "source": [
        "result = re.split(r'([,.]|\\s)', text)\n",
        "\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd7c386b",
      "metadata": {},
      "source": [
        "- The split introduces empty strings\n",
        "- Filter them out to keep only meaningful tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f7cac8f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Strip whitespace from each item and then filter out any empty strings.\n",
        "result = [item for item in result if item.strip()]\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66d4419b",
      "metadata": {},
      "source": [
        "- Expand the pattern to cover more punctuation (e.g., ?, !, quotes)\n",
        "- This should produce cleaner, more realistic token streams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d0c7456",
      "metadata": {},
      "outputs": [],
      "source": [
        "text = \"Hello, world. Is this-- a test?\"\n",
        "\n",
        "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "result = [item.strip() for item in result if item.strip()]\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e995101f",
      "metadata": {},
      "source": [
        "- The regex looks good enough for a first pass\n",
        "- Now apply it to the full text loaded earlier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08fb54fa",
      "metadata": {},
      "outputs": [],
      "source": [
        "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
        "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "print(preprocessed[:30])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30f685d2",
      "metadata": {},
      "source": [
        "- Count the total number of tokens produced"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2cc6106",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(len(preprocessed))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59869dc9",
      "metadata": {},
      "source": [
        "## 2.3 Converting tokens into IDs\n",
        "\n",
        "- Models need numbers, not strings\n",
        "- Build a vocabulary mapping each unique token to an integer ID\n",
        "- Start by listing unique tokens in a consistent order"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e72af4a",
      "metadata": {},
      "outputs": [],
      "source": [
        "all_words = sorted(list(set(preprocessed)))\n",
        "vocab_size = len(all_words)\n",
        "\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d93a541",
      "metadata": {},
      "source": [
        "- Create the token → ID dictionary using a simple enumeration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f5b5dc7",
      "metadata": {},
      "outputs": [],
      "source": [
        "vocab = {token:integer for integer,token in enumerate(all_words)}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "062fc077",
      "metadata": {},
      "source": [
        "- Peek at a handful of entries to sanity-check the mapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c2b7dba",
      "metadata": {},
      "outputs": [],
      "source": [
        "for i, item in enumerate(vocab.items()):\n",
        "    print(item)\n",
        "    if i >= 50:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a683ace",
      "metadata": {},
      "source": [
        "- Next, I’ll wrap the logic into a small tokenizer class\n",
        "- Then I’ll test it on a short sample string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e5d03a3",
      "metadata": {},
      "outputs": [],
      "source": [
        "class SimpleTokenizerV1:\n",
        "    def __init__(self, vocab):\n",
        "        self.str_to_int = vocab\n",
        "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
        "    \n",
        "    def encode(self, text):\n",
        "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "                                \n",
        "        preprocessed = [\n",
        "            item.strip() for item in preprocessed if item.strip()\n",
        "        ]\n",
        "        ids = [self.str_to_int[s] for s in preprocessed]\n",
        "        return ids\n",
        "        \n",
        "    def decode(self, ids):\n",
        "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "        # Replace spaces before the specified punctuations\n",
        "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "        return text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62e87496",
      "metadata": {},
      "source": [
        "- `encode` converts text into token IDs\n",
        "- `decode` converts token IDs back into text\n",
        "- These IDs are what we’ll later feed into embeddings for the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f793248",
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = SimpleTokenizerV1(vocab)\n",
        "\n",
        "text = \"\"\"\"It's the last he painted, you know,\" \n",
        "           Mrs. Gisburn said with pardonable pride.\"\"\"\n",
        "ids = tokenizer.encode(text)\n",
        "print(ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c7ea35b",
      "metadata": {},
      "source": [
        "- Decode the IDs to verify the round-trip works"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3150c9c",
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer.decode(ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1400536a",
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer.decode(tokenizer.encode(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60a82965",
      "metadata": {},
      "source": [
        "## 2.4 Adding special context tokens\n",
        "\n",
        "- It's useful to add some \"special\" tokens for unknown words and to denote the end of a text\n",
        "- Some tokenizers use special tokens to help the LLM with additional context\n",
        "- Some of these special tokens are\n",
        "  - `[BOS]` (beginning of sequence) marks the beginning of text\n",
        "  - `[EOS]` (end of sequence) marks where the text ends (this is usually used to concatenate multiple unrelated texts, e.g., two different Wikipedia articles or two different books, and so on)\n",
        "  - `[PAD]` (padding) if we train LLMs with a batch size greater than 1 (we may include multiple texts with different lengths; with the padding token we pad the shorter texts to the longest length so that all texts have an equal length)\n",
        "- `[UNK]` to represent words that are not included in the vocabulary\n",
        "\n",
        "- Note that GPT-2 does not need any of these tokens mentioned above but only uses an `<|endoftext|>` token to reduce complexity\n",
        "- The `<|endoftext|>` is analogous to the `[EOS]` token mentioned above\n",
        "- GPT also uses the `<|endoftext|>` for padding (since we typically use a mask when training on batched inputs, we would not attend padded tokens anyways, so it does not matter what these tokens are)\n",
        "- GPT-2 does not use an `<UNK>` token for out-of-vocabulary words; instead, GPT-2 uses a byte-pair encoding (BPE) tokenizer, which breaks down words into subword units which we will discuss in a later section\n",
        "- We use the `<|endoftext|>` tokens between two independent sources of text:\n",
        "- Let's see what happens if we tokenize the following text:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41c7554a",
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = SimpleTokenizerV1(vocab)\n",
        "\n",
        "text = \"Hello, do you like tea. Is this-- a test?\"\n",
        "\n",
        "tokenizer.encode(text)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "llms-from-scratch-practice (3.12.10)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
