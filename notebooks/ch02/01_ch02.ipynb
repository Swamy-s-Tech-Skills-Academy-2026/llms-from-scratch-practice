{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c2b409d3",
      "metadata": {},
      "source": [
        "# Chapter 2: Working with Text Data\n",
        "\n",
        "- Learning how to prepare and process text data for language modeling\n",
        "- Understanding tokenization and vocabulary creation\n",
        "- Converting raw text into a format suitable for model training"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f374a3f",
      "metadata": {},
      "source": [
        "Packages that are being used in this notebook:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e68bf503",
      "metadata": {},
      "outputs": [],
      "source": [
        "from importlib.metadata import version\n",
        "\n",
        "print(\"torch version:\", version(\"torch\"))\n",
        "print(\"tiktoken version:\", version(\"tiktoken\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85d3ff76",
      "metadata": {},
      "source": [
        "## 2.1 Understanding word embeddings\n",
        "\n",
        "- Embeddings can represent different types of data, but for LLMs we primarily work with text embeddings\n",
        "- Language models operate in high-dimensional vector spaces (typically hundreds or thousands of dimensions)\n",
        "- Visualizing these spaces is challenging since we naturally think in 1-3 dimensions, so simplified 2D representations help build intuition"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28079f01",
      "metadata": {},
      "source": [
        "## 2.2 Tokenizing text\n",
        "\n",
        "- Tokenization involves splitting text into discrete units (words, punctuation, subwords)\n",
        "- These tokens become the basic building blocks that our model will process\n",
        "- We'll use a public domain text: [The Verdict by Edith Wharton](https://en.wikisource.org/wiki/The_Verdict) as our sample data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4dc50a0d",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Check for local file first (zero-copy policy compliance)\n",
        "# The file should already exist in the notebook directory\n",
        "file_path = \"the-verdict.txt\"\n",
        "\n",
        "if not os.path.exists(file_path):\n",
        "    # Fallback: download from public domain source if local file missing\n",
        "    # Note: This is a public domain text by Edith Wharton\n",
        "    import requests\n",
        "    url = (\n",
        "        \"https://raw.githubusercontent.com/rasbt/\"\n",
        "        \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
        "        \"the-verdict.txt\"\n",
        "    )\n",
        "    response = requests.get(url, timeout=30)\n",
        "    response.raise_for_status()\n",
        "    with open(file_path, \"wb\") as f:\n",
        "        f.write(response.content)\n",
        "    print(f\"Downloaded {file_path} from external source\")\n",
        "else:\n",
        "    print(f\"Using local file: {file_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8027639",
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "    \n",
        "print(\"Total number of character:\", len(raw_text))\n",
        "print(raw_text[:99])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "149d1ab5",
      "metadata": {},
      "source": [
        "- Our objective: convert the raw text into tokens that can be processed by a language model\n",
        "- We'll start by building a basic tokenizer using simple example text, then apply it to our full dataset\n",
        "- First, let's see how a basic regex pattern can split text on whitespace characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2679e062",
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "text = \"Hello, world. This, is a test.\"\n",
        "result = re.split(r'(\\s)', text)\n",
        "\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a3e334c",
      "metadata": {},
      "source": [
        "- Splitting only on whitespace isn't sufficient - we also need to handle punctuation like commas and periods\n",
        "- Let's enhance our regex pattern to capture these punctuation marks as separate tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7eec2c50",
      "metadata": {},
      "outputs": [],
      "source": [
        "result = re.split(r'([,.]|\\s)', text)\n",
        "\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd7c386b",
      "metadata": {},
      "source": [
        "- The current approach produces empty strings in our token list\n",
        "- We need to filter these out to get clean, meaningful tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f7cac8f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Strip whitespace from each item and then filter out any empty strings.\n",
        "result = [item for item in result if item.strip()]\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66d4419b",
      "metadata": {},
      "source": [
        "- The tokenizer is improving, but we should expand it to handle more punctuation types\n",
        "- Let's add support for question marks, exclamation marks, and other common punctuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d0c7456",
      "metadata": {},
      "outputs": [],
      "source": [
        "text = \"Hello, world. Is this-- a test?\"\n",
        "\n",
        "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "result = [item.strip() for item in result if item.strip()]\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e995101f",
      "metadata": {},
      "source": [
        "- Our tokenizer pattern looks solid now\n",
        "- Time to apply it to the full text we loaded earlier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08fb54fa",
      "metadata": {},
      "outputs": [],
      "source": [
        "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
        "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "print(preprocessed[:30])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30f685d2",
      "metadata": {},
      "source": [
        "- Now let's count how many tokens we've extracted from the text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2cc6106",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(len(preprocessed))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59869dc9",
      "metadata": {},
      "source": [
        "## 2.3 Converting tokens into IDs\n",
        "\n",
        "- To work with tokens computationally, we need to map them to numeric IDs\n",
        "- We'll build a vocabulary dictionary that assigns each unique token a unique integer identifier\n",
        "- First step: extract all unique tokens and organize them alphabetically"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e72af4a",
      "metadata": {},
      "outputs": [],
      "source": [
        "all_words = sorted(list(set(preprocessed)))\n",
        "vocab_size = len(all_words)\n",
        "\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d93a541",
      "metadata": {},
      "source": [
        "- Now we'll construct the vocabulary mapping: each token gets assigned a sequential integer ID"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f5b5dc7",
      "metadata": {},
      "outputs": [],
      "source": [
        "vocab = {token:integer for integer,token in enumerate(all_words)}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "062fc077",
      "metadata": {},
      "source": [
        "- Let's inspect the first 50 token-to-ID mappings to see how our vocabulary looks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c2b7dba",
      "metadata": {},
      "outputs": [],
      "source": [
        "for i, item in enumerate(vocab.items()):\n",
        "    print(item)\n",
        "    if i >= 50:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a683ace",
      "metadata": {},
      "source": [
        "- Below, we illustrate the tokenization of a short sample text using a small vocabulary:\n",
        "- Putting it now all together into a tokenizer class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "3e5d03a3",
      "metadata": {},
      "outputs": [],
      "source": [
        "class SimpleTokenizerV1:\n",
        "    def __init__(self, vocab):\n",
        "        self.str_to_int = vocab\n",
        "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
        "    \n",
        "    def encode(self, text):\n",
        "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "                                \n",
        "        preprocessed = [\n",
        "            item.strip() for item in preprocessed if item.strip()\n",
        "        ]\n",
        "        ids = [self.str_to_int[s] for s in preprocessed]\n",
        "        return ids\n",
        "        \n",
        "    def decode(self, ids):\n",
        "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "        # Replace spaces before the specified punctuations\n",
        "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "        return text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62e87496",
      "metadata": {},
      "source": [
        "- The `encode` function turns text into token IDs\n",
        "- The `decode` function turns token IDs back into text\n",
        "- We can use the tokenizer to encode (that is, tokenize) texts into integers\n",
        "- These integers can then be embedded (later) as input of/for the LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "1f793248",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
          ]
        }
      ],
      "source": [
        "tokenizer = SimpleTokenizerV1(vocab)\n",
        "\n",
        "text = \"\"\"\"It's the last he painted, you know,\" \n",
        "           Mrs. Gisburn said with pardonable pride.\"\"\"\n",
        "ids = tokenizer.encode(text)\n",
        "print(ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c7ea35b",
      "metadata": {},
      "source": [
        "- We can decode the integers back into text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "d3150c9c",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\" It\\' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.decode(ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "1400536a",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\" It\\' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.decode(tokenizer.encode(text))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "llms-from-scratch-practice (3.12.10)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
