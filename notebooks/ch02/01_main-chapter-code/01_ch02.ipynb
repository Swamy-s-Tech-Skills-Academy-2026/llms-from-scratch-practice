{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c2b409d3",
      "metadata": {},
      "source": [
        "# Chapter 2: Working with Text Data\n",
        "\n",
        "- I'm learning how to prepare and clean text for language modeling\n",
        "- My goal: understand tokenization and build a small vocabulary\n",
        "- I want to turn raw text into numeric inputs I can train on"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f374a3f",
      "metadata": {},
      "source": [
        "Packages used in this notebook:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e68bf503",
      "metadata": {},
      "outputs": [],
      "source": [
        "from importlib.metadata import version\n",
        "\n",
        "print(\"torch version:\", version(\"torch\"))\n",
        "print(\"tiktoken version:\", version(\"tiktoken\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85d3ff76",
      "metadata": {},
      "source": [
        "## 2.1 Understanding word embeddings\n",
        "\n",
        "- I'm learning that for LLMs, tokens are represented as vectors in a high-dimensional space\n",
        "- These vectors encode relationships that the model can learn\n",
        "- I find it helpful to use 2D sketches to build intuition about this much larger space"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28079f01",
      "metadata": {},
      "source": [
        "## 2.2 Tokenizing text\n",
        "\n",
        "- Tokenization splits text into units (words, punctuation, or subwords)\n",
        "- These units become the inputs the model will work with\n",
        "- I’ll use a public-domain text (The Verdict) as sample data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4dc50a0d",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Check for local file first (zero-copy policy compliance)\n",
        "# The file should already exist in the notebook directory\n",
        "file_path = \"the-verdict.txt\"\n",
        "\n",
        "if not os.path.exists(file_path):\n",
        "    # Fallback: download from public domain source if local file missing\n",
        "    # Note: This is a public domain text by Edith Wharton\n",
        "    import requests\n",
        "    url = (\n",
        "        \"https://raw.githubusercontent.com/rasbt/\"\n",
        "        \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
        "        \"the-verdict.txt\"\n",
        "    )\n",
        "    response = requests.get(url, timeout=30)\n",
        "    response.raise_for_status()\n",
        "    with open(file_path, \"wb\") as f:\n",
        "        f.write(response.content)\n",
        "    print(f\"Downloaded {file_path} from external source\")\n",
        "else:\n",
        "    print(f\"Using local file: {file_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8027639",
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "    \n",
        "print(\"Total number of character:\", len(raw_text))\n",
        "print(raw_text[:99])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "149d1ab5",
      "metadata": {},
      "source": [
        "- Goal: turn the raw text into tokens a model can consume\n",
        "- I’ll build a simple regex tokenizer first, then scale it to the full text\n",
        "- Start by splitting on whitespace to see the baseline behavior"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2679e062",
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "text = \"Hello, world. This, is a test.\"\n",
        "result = re.split(r'(\\s)', text)\n",
        "\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a3e334c",
      "metadata": {},
      "source": [
        "- I noticed that whitespace-only splitting misses punctuation boundaries\n",
        "- My next step: treat commas and periods as their own tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7eec2c50",
      "metadata": {},
      "outputs": [],
      "source": [
        "result = re.split(r'([,.]|\\s)', text)\n",
        "\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd7c386b",
      "metadata": {},
      "source": [
        "- I noticed the split introduces empty strings\n",
        "- I'll filter them out to keep only meaningful tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f7cac8f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Strip whitespace from each item and then filter out any empty strings.\n",
        "result = [item for item in result if item.strip()]\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66d4419b",
      "metadata": {},
      "source": [
        "- I'm expanding the pattern to cover more punctuation (e.g., ?, !, quotes)\n",
        "- This should produce cleaner, more realistic token streams for my model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d0c7456",
      "metadata": {},
      "outputs": [],
      "source": [
        "text = \"Hello, world. Is this-- a test?\"\n",
        "\n",
        "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "result = [item.strip() for item in result if item.strip()]\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e995101f",
      "metadata": {},
      "source": [
        "- The regex looks good enough for a first pass\n",
        "- Now I'll apply it to the full text I loaded earlier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08fb54fa",
      "metadata": {},
      "outputs": [],
      "source": [
        "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
        "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "print(preprocessed[:30])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30f685d2",
      "metadata": {},
      "source": [
        "- Let me count the total number of tokens I produced"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2cc6106",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(len(preprocessed))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59869dc9",
      "metadata": {},
      "source": [
        "## 2.3 Converting tokens into IDs\n",
        "\n",
        "- I'm learning that models need numbers, not strings\n",
        "- I'll build a vocabulary mapping each unique token to an integer ID\n",
        "- Starting by listing unique tokens in a consistent order"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e72af4a",
      "metadata": {},
      "outputs": [],
      "source": [
        "all_words = sorted(list(set(preprocessed)))\n",
        "vocab_size = len(all_words)\n",
        "\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d93a541",
      "metadata": {},
      "source": [
        "- I'll create the token → ID dictionary using a simple enumeration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f5b5dc7",
      "metadata": {},
      "outputs": [],
      "source": [
        "vocab = {token:integer for integer,token in enumerate(all_words)}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "062fc077",
      "metadata": {},
      "source": [
        "- Let me peek at a handful of entries to sanity-check my mapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c2b7dba",
      "metadata": {},
      "outputs": [],
      "source": [
        "for i, item in enumerate(vocab.items()):\n",
        "    print(item)\n",
        "    if i >= 50:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a683ace",
      "metadata": {},
      "source": [
        "- Next, I’ll wrap the logic into a small tokenizer class\n",
        "- Then I’ll test it on a short sample string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e5d03a3",
      "metadata": {},
      "outputs": [],
      "source": [
        "class SimpleTokenizerV1:\n",
        "    def __init__(self, vocab):\n",
        "        self.str_to_int = vocab\n",
        "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
        "    \n",
        "    def encode(self, text):\n",
        "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "                                \n",
        "        preprocessed = [\n",
        "            item.strip() for item in preprocessed if item.strip()\n",
        "        ]\n",
        "        ids = [self.str_to_int[s] for s in preprocessed]\n",
        "        return ids\n",
        "        \n",
        "    def decode(self, ids):\n",
        "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "        # Replace spaces before the specified punctuations\n",
        "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "        return text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62e87496",
      "metadata": {},
      "source": [
        "- `encode` converts text into token IDs\n",
        "- `decode` converts token IDs back into text\n",
        "- These IDs are what we’ll later feed into embeddings for the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f793248",
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = SimpleTokenizerV1(vocab)\n",
        "\n",
        "text = \"\"\"\"It's the last he painted, you know,\" \n",
        "           Mrs. Gisburn said with pardonable pride.\"\"\"\n",
        "ids = tokenizer.encode(text)\n",
        "print(ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c7ea35b",
      "metadata": {},
      "source": [
        "- I'll decode the IDs to verify the round-trip works"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3150c9c",
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer.decode(ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1400536a",
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer.decode(tokenizer.encode(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60a82965",
      "metadata": {},
      "source": [
        "## 2.4 Adding special context tokens\n",
        "\n",
        "- I noticed a problem: when I try to tokenize text with words not in my vocabulary, it breaks\n",
        "- I need a way to handle unknown words and mark text boundaries\n",
        "- I'm learning about special tokens that can help with this\n",
        "\n",
        "My understanding so far:\n",
        "- `[UNK]` can represent words not in vocabulary (handles unknown words)\n",
        "- `[EOS]` marks end of text (useful when concatenating multiple texts)\n",
        "- `[PAD]` is for padding shorter texts to match longer ones in batches\n",
        "- `[BOS]` marks beginning of sequence (some tokenizers use this)\n",
        "\n",
        "I'm curious about GPT-2's approach:\n",
        "- GPT-2 uses `<|endoftext|>` instead of all those tokens - much simpler!\n",
        "- It's like `[EOS]` but also used for padding (since we mask padded tokens anyway)\n",
        "- GPT-2 doesn't use `[UNK]` because it uses BPE (byte-pair encoding) which breaks words into subword units\n",
        "- I'll learn about BPE later, but for now I'll add `<|unk|>` and `<|endoftext|>` to handle these cases\n",
        "\n",
        "My takeaway: GPT-2's simpler approach makes sense - one token does multiple jobs when you use masking properly.\n",
        "\n",
        "Let me see what happens when I try to tokenize text with words not in my vocabulary:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41c7554a",
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = SimpleTokenizerV1(vocab)\n",
        "\n",
        "text = \"Hello, do you like tea. Is this-- a test?\"\n",
        "\n",
        "try:\n",
        "    tokenizer.encode(text)\n",
        "except KeyError as e:\n",
        "    print(f\"KeyError: The token {e} is not in the vocabulary.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78997cb2",
      "metadata": {},
      "source": [
        "- I got an error because \"Hello\" isn't in my vocabulary\n",
        "- To handle this, I'll add special tokens like `\"<|unk|>\"` to represent unknown words\n",
        "- Since I'm extending the vocabulary anyway, I'll also add `\"<|endoftext|>\"` which GPT-2 uses to mark the end of text (and between concatenated texts when training on multiple articles or books)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebcb1258",
      "metadata": {},
      "outputs": [],
      "source": [
        "all_tokens = sorted(list(set(preprocessed)))\n",
        "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
        "\n",
        "vocab = {token:integer for integer,token in enumerate(all_tokens)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50f8aa69",
      "metadata": {},
      "outputs": [],
      "source": [
        "len(vocab.items())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e187d11d",
      "metadata": {},
      "outputs": [],
      "source": [
        "for i, item in enumerate(list(vocab.items())[-5:]):\n",
        "    print(item)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7eb901db",
      "metadata": {},
      "source": [
        "- I also need to adjust my tokenizer so it knows when and how to use the new `<|unk|>` token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5ad8bbe",
      "metadata": {},
      "outputs": [],
      "source": [
        "class SimpleTokenizerV2:\n",
        "    def __init__(self, vocab):\n",
        "        self.str_to_int = vocab\n",
        "        self.int_to_str = { i:s for s,i in vocab.items()}\n",
        "    \n",
        "    def encode(self, text):\n",
        "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "        preprocessed = [\n",
        "            item if item in self.str_to_int \n",
        "            else \"<|unk|>\" for item in preprocessed\n",
        "        ]\n",
        "\n",
        "        ids = [self.str_to_int[s] for s in preprocessed]\n",
        "        return ids\n",
        "        \n",
        "    def decode(self, ids):\n",
        "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "        # Replace spaces before the specified punctuations\n",
        "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
        "        return text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c315157",
      "metadata": {},
      "source": [
        "Now let me test the modified tokenizer with some text that includes unknown words and multiple sentences:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8dfa4d24",
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = SimpleTokenizerV2(vocab)\n",
        "\n",
        "text1 = \"Hello, do you like tea?\"\n",
        "text2 = \"In the sunlit terraces of the palace.\"\n",
        "\n",
        "text = \" <|endoftext|> \".join((text1, text2))\n",
        "\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9bde947",
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer.encode(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b00511b9",
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer.decode(tokenizer.encode(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdcc957a",
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = SimpleTokenizerV2(vocab)\n",
        "\n",
        "text = \"\"\"\"It's the last he painted, you know,\" \n",
        "           Mrs. Gisburn said with pardonable pride.\"\"\"\n",
        "ids = tokenizer.encode(text)\n",
        "print(ids)\n",
        "tokenizer.decode(ids)\n",
        "tokenizer.decode(tokenizer.encode(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e1be18c",
      "metadata": {},
      "source": [
        "## 2.5 BytePair encoding\n",
        "\n",
        "- GPT-2 used BytePair encoding (BPE) as its tokenizer\n",
        "- it allows the model to break down words that aren't in its predefined vocabulary into smaller subword units or even individual characters, enabling it to handle out-of-vocabulary words\n",
        "- For instance, if GPT-2's vocabulary doesn't have the word \"unfamiliarword,\" it might tokenize it as [\"unfam\", \"iliar\", \"word\"] or some other subword breakdown, depending on its trained BPE merges\n",
        "- The original BPE tokenizer can be found here: [https://github.com/openai/gpt-2/blob/master/src/encoder.py](https://github.com/openai/gpt-2/blob/master/src/encoder.py)\n",
        "- In this chapter, we are using the BPE tokenizer from OpenAI's open-source [tiktoken](https://github.com/openai/tiktoken) library, which implements its core algorithms in Rust to improve computational performance\n",
        "- I created a notebook in the [./bytepair_encoder](../02_bonus_bytepair-encoder) that compares these two implementations side-by-side (tiktoken was about 5x faster on the sample text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c08bad2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "538259e0",
      "metadata": {},
      "outputs": [],
      "source": [
        "import importlib\n",
        "import tiktoken\n",
        "\n",
        "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf92e55d",
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60749936",
      "metadata": {},
      "outputs": [],
      "source": [
        "text = (\n",
        "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
        "     \"of someunknownPlace.\"\n",
        ")\n",
        "\n",
        "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "print(integers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71228dfe",
      "metadata": {},
      "outputs": [],
      "source": [
        "strings = tokenizer.decode(integers)\n",
        "\n",
        "print(strings)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "llms-from-scratch-practice (3.12.10)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
