{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2b409d3",
   "metadata": {},
   "source": [
    "# Chapter 2: Working with Text Data\n",
    "\n",
    "- I'm learning how to prepare and clean text for language modeling\n",
    "- My goal: understand tokenization and build a small vocabulary\n",
    "- I want to turn raw text into numeric inputs I can train on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f374a3f",
   "metadata": {},
   "source": [
    "Packages used in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68bf503",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "print(\"torch version:\", version(\"torch\"))\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d3ff76",
   "metadata": {},
   "source": [
    "## 2.1 Understanding word embeddings\n",
    "\n",
    "- I'm learning that for LLMs, tokens are represented as vectors in a high-dimensional space\n",
    "- These vectors encode relationships that the model can learn\n",
    "- I find it helpful to use 2D sketches to build intuition about this much larger space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28079f01",
   "metadata": {},
   "source": [
    "## 2.2 Tokenizing text\n",
    "\n",
    "- Tokenization splits text into units (words, punctuation, or subwords)\n",
    "- These units become the inputs the model will work with\n",
    "- I’ll use a public-domain text (The Verdict) as sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc50a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Check for local file first (zero-copy policy compliance)\n",
    "# The file should already exist in the notebook directory\n",
    "file_path = \"the-verdict.txt\"\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    # Fallback: download from public domain source if local file missing\n",
    "    # Note: This is a public domain text by Edith Wharton\n",
    "    import requests\n",
    "    url = (\n",
    "        \"https://raw.githubusercontent.com/rasbt/\"\n",
    "        \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
    "        \"the-verdict.txt\"\n",
    "    )\n",
    "    response = requests.get(url, timeout=30)\n",
    "    response.raise_for_status()\n",
    "    with open(file_path, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    print(f\"Downloaded {file_path} from external source\")\n",
    "else:\n",
    "    print(f\"Using local file: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8027639",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "    \n",
    "print(\"Total number of character:\", len(raw_text))\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149d1ab5",
   "metadata": {},
   "source": [
    "- Goal: turn the raw text into tokens a model can consume\n",
    "- I’ll build a simple regex tokenizer first, then scale it to the full text\n",
    "- Start by splitting on whitespace to see the baseline behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2679e062",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "text = \"Hello, world. This, is a test.\"\n",
    "result = re.split(r'(\\s)', text)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3e334c",
   "metadata": {},
   "source": [
    "- I noticed that whitespace-only splitting misses punctuation boundaries\n",
    "- My next step: treat commas and periods as their own tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eec2c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = re.split(r'([,.]|\\s)', text)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7c386b",
   "metadata": {},
   "source": [
    "- I noticed the split introduces empty strings\n",
    "- I'll filter them out to keep only meaningful tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7cac8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip whitespace from each item and then filter out any empty strings.\n",
    "result = [item for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d4419b",
   "metadata": {},
   "source": [
    "- I'm expanding the pattern to cover more punctuation (e.g., ?, !, quotes)\n",
    "- This should produce cleaner, more realistic token streams for my model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0c7456",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hello, world. Is this-- a test?\"\n",
    "\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "result = [item.strip() for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e995101f",
   "metadata": {},
   "source": [
    "- The regex looks good enough for a first pass\n",
    "- Now I'll apply it to the full text I loaded earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fb54fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(preprocessed[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f685d2",
   "metadata": {},
   "source": [
    "- Let me count the total number of tokens I produced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cc6106",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(preprocessed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59869dc9",
   "metadata": {},
   "source": [
    "## 2.3 Converting tokens into IDs\n",
    "\n",
    "- I'm learning that models need numbers, not strings\n",
    "- I'll build a vocabulary mapping each unique token to an integer ID\n",
    "- Starting by listing unique tokens in a consistent order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e72af4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = sorted(list(set(preprocessed)))\n",
    "vocab_size = len(all_words)\n",
    "\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d93a541",
   "metadata": {},
   "source": [
    "- I'll create the token → ID dictionary using a simple enumeration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5b5dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {token:integer for integer,token in enumerate(all_words)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062fc077",
   "metadata": {},
   "source": [
    "- Let me peek at a handful of entries to sanity-check my mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2b7dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >= 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a683ace",
   "metadata": {},
   "source": [
    "- Next, I’ll wrap the logic into a small tokenizer class\n",
    "- Then I’ll test it on a short sample string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5d03a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "                                \n",
    "        preprocessed = [\n",
    "            item.strip() for item in preprocessed if item.strip()\n",
    "        ]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e87496",
   "metadata": {},
   "source": [
    "- `encode` converts text into token IDs\n",
    "- `decode` converts token IDs back into text\n",
    "- These IDs are what we’ll later feed into embeddings for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f793248",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "\n",
    "text = \"\"\"\"It's the last he painted, you know,\" \n",
    "           Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7ea35b",
   "metadata": {},
   "source": [
    "- I'll decode the IDs to verify the round-trip works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3150c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1400536a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a82965",
   "metadata": {},
   "source": [
    "## 2.4 Adding special context tokens\n",
    "\n",
    "- I noticed a problem: when I try to tokenize text with words not in my vocabulary, it breaks\n",
    "- I need a way to handle unknown words and mark text boundaries\n",
    "- I'm learning about special tokens that can help with this\n",
    "\n",
    "My understanding so far:\n",
    "- `[UNK]` can represent words not in vocabulary (handles unknown words)\n",
    "- `[EOS]` marks end of text (useful when concatenating multiple texts)\n",
    "- `[PAD]` is for padding shorter texts to match longer ones in batches\n",
    "- `[BOS]` marks beginning of sequence (some tokenizers use this)\n",
    "\n",
    "I'm curious about GPT-2's approach:\n",
    "- GPT-2 uses `<|endoftext|>` instead of all those tokens - much simpler!\n",
    "- It's like `[EOS]` but also used for padding (since we mask padded tokens anyway)\n",
    "- GPT-2 doesn't use `[UNK]` because it uses BPE (byte-pair encoding) which breaks words into subword units\n",
    "- I'll learn about BPE later, but for now I'll add `<|unk|>` and `<|endoftext|>` to handle these cases\n",
    "\n",
    "My takeaway: GPT-2's simpler approach makes sense - one token does multiple jobs when you use masking properly.\n",
    "\n",
    "Let me see what happens when I try to tokenize text with words not in my vocabulary:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c7554a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "\n",
    "text = \"Hello, do you like tea. Is this-- a test?\"\n",
    "\n",
    "try:\n",
    "    tokenizer.encode(text)\n",
    "except KeyError as e:\n",
    "    print(f\"KeyError: The token {e} is not in the vocabulary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78997cb2",
   "metadata": {},
   "source": [
    "- I got an error because \"Hello\" isn't in my vocabulary\n",
    "- To handle this, I'll add special tokens like `\"<|unk|>\"` to represent unknown words\n",
    "- Since I'm extending the vocabulary anyway, I'll also add `\"<|endoftext|>\"` which GPT-2 uses to mark the end of text (and between concatenated texts when training on multiple articles or books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcb1258",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "\n",
    "vocab = {token:integer for integer,token in enumerate(all_tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f8aa69",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocab.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e187d11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb901db",
   "metadata": {},
   "source": [
    "- I also need to adjust my tokenizer so it knows when and how to use the new `<|unk|>` token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ad8bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = { i:s for s,i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [\n",
    "            item if item in self.str_to_int \n",
    "            else \"<|unk|>\" for item in preprocessed\n",
    "        ]\n",
    "\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c315157",
   "metadata": {},
   "source": [
    "Now let me test the modified tokenizer with some text that includes unknown words and multiple sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfa4d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bde947",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00511b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcc957a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "\n",
    "text = \"\"\"\"It's the last he painted, you know,\" \n",
    "           Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)\n",
    "tokenizer.decode(ids)\n",
    "tokenizer.decode(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1be18c",
   "metadata": {},
   "source": [
    "## 2.5 BytePair encoding\n",
    "\n",
    "- I'm learning that GPT-2 used BytePair encoding (BPE) as its tokenizer\n",
    "- BPE lets the model break words that aren't in its vocabulary into smaller subword units or even characters, so it can handle out-of-vocabulary words\n",
    "- For example, if the vocabulary doesn't have \"unfamiliarword,\" it might tokenize it as something like [\"unfam\", \"iliar\", \"word\"] depending on trained BPE merges\n",
    "- The original BPE tokenizer is here: [OpenAI encoder.py](https://github.com/openai/gpt-2/blob/master/src/encoder.py)\n",
    "- In this chapter I'm using the BPE tokenizer from OpenAI's [tiktoken](https://github.com/openai/tiktoken) library, which implements the core algorithms in Rust for better performance\n",
    "- I created a notebook in [./bytepair_encoder](../02_bonus_bytepair-encoder) that compares the two implementations side-by-side (tiktoken was about 5x faster on my sample text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c08bad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538259e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import tiktoken\n",
    "\n",
    "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf92e55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60749936",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = (\n",
    "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    "     \"of someunknownPlace.\"\n",
    ")\n",
    "\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71228dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "strings = tokenizer.decode(integers)\n",
    "\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741ed033",
   "metadata": {},
   "source": [
    "## 2.6 Data sampling with a sliding window\n",
    "\n",
    "- I'm learning that we train LLMs to generate one token at a time, so I need to prepare training data where the next token in a sequence is the target to predict:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924b459d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(f\"Encoded Text: {enc_text[:100]}\")\n",
    "print(f\"Total Tokens: {len(enc_text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad1f63a",
   "metadata": {},
   "source": [
    "- For each text chunk, I need inputs and targets\n",
    "- Since the model predicts the next token, the targets are the inputs shifted by one position to the right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85272146",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_sample = enc_text[50:]\n",
    "print(\"Encoded sample:\", enc_sample)\n",
    "dec_sample = tokenizer.decode(enc_sample)\n",
    "print(\"Decoded sample:\", dec_sample[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2430d7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_size = 4\n",
    "\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:      {y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2193888",
   "metadata": {},
   "source": [
    "- One by one, the prediction looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d0a40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "\n",
    "    print(context, \"---->\", desired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb21634",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "\n",
    "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fd9a29",
   "metadata": {},
   "source": [
    "- I'll tackle next-token prediction in a later chapter after covering the attention mechanism\n",
    "- For now, I'm implementing a simple data loader that iterates over the input and returns input/target pairs shifted by one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67fb89f",
   "metadata": {},
   "source": [
    "- I'm using PyTorch here (see Appendix A for installation tips if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482c13ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a8216f",
   "metadata": {},
   "source": [
    "- I'm using a sliding window: move the window by +1 each time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9652c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "        assert len(token_ids) > max_length, \"Number of tokenized inputs must at least be equal to max_length+1\"\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eaf1c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, \n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605ee04f",
   "metadata": {},
   "source": [
    "- Let me test the dataloader with batch size 1 and context size 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164dbed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6e2f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fe3777",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_batch = next(data_iter)\n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046a9d25",
   "metadata": {},
   "source": [
    "- Example with stride equal to context length (here: 4):\n",
    "- I can also create batched outputs\n",
    "- I'm using stride = context length here so batches don't overlap; more overlap can lead to overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45ede78",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets)\n",
    "\n",
    "# Verify shapes are correct\n",
    "assert inputs.shape == (8, 4), f\"Expected shape (8, 4), got {inputs.shape}\"\n",
    "assert targets.shape == (8, 4), f\"Expected shape (8, 4), got {targets.shape}\"\n",
    "\n",
    "print(\"\\n✓ Shape verification passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96f2e12",
   "metadata": {},
   "source": [
    "## 2.7 Creating token embeddings\n",
    "\n",
    "- I'm learning that token IDs are still just discrete integers - I need to convert them to continuous vectors\n",
    "- These vectors (embeddings) will be what the LLM actually works with\n",
    "- I understand that embeddings are trainable - they get updated during model training\n",
    "- Let me practice with a simple example to build intuition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15ce4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting with a simple example: 4 token IDs\n",
    "input_ids = torch.tensor([2, 3, 5, 1])\n",
    "print(\"Input IDs:\", input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505fc7ca",
   "metadata": {},
   "source": [
    "- I'll pretend I have a vocabulary of only 6 words and want 3-dimensional embeddings\n",
    "- This is much smaller than reality, but it helps me visualize what's happening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e8a956",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 6\n",
    "output_dim = 3\n",
    "\n",
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "\n",
    "print(\"Embedding layer :\\n\", embedding_layer)\n",
    "print(\"Embedding layer weights:\\n\", embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1302973a",
   "metadata": {},
   "source": [
    "- The embedding layer creates a 6×3 weight matrix (6 vocab words, 3 dimensions each)\n",
    "- Let me look at what these initial random weights look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36b4ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa2987e",
   "metadata": {},
   "source": [
    "- I'm curious about how this works internally\n",
    "- I noticed that embedding layers are essentially efficient lookup operations\n",
    "- I learned there's a bonus notebook at [../03_bonus_embedding-vs-matmul](../03_bonus_embedding-vs-matmul) that explains how embeddings relate to one-hot encoding + matrix multiplication\n",
    "- My understanding: embeddings are trainable neural network layers optimized via backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f234330f",
   "metadata": {},
   "source": [
    "- Let me test: convert token ID 3 into its 3-dimensional vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9dc5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embedding_layer(torch.tensor([3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8550e1d9",
   "metadata": {},
   "source": [
    "- I noticed this is exactly the 4th row (index 3) from the embedding weight matrix!\n",
    "- Now let me embed all four input_ids at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "920853aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-2.8400, -0.7849, -1.4096],\n",
      "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8878eeeb",
   "metadata": {},
   "source": [
    "- My key insight: an embedding layer is essentially a lookup table\n",
    "- Each token ID points to its corresponding row in the weight matrix\n",
    "\n",
    "**I want to explore more:** I'm curious about how embeddings compare to traditional one-hot encoding + linear layers. I'll check out the bonus content at [../03_bonus_embedding-vs-matmul](../03_bonus_embedding-vs-matmul) later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fb04d5",
   "metadata": {},
   "source": [
    "## 2.8 Encoding word positions\n",
    "\n",
    "- I'm realizing there's a problem: embeddings convert token IDs to vectors, but they don't capture **where** the token appears in the sequence\n",
    "- If I have \"cat chased mouse\" vs \"mouse chased cat\", the token embeddings are identical - the model loses position information!\n",
    "- I'm learning about **positional embeddings** - they encode the position of each token\n",
    "- The model combines token embeddings + positional embeddings to get the final input representation\n",
    "\n",
    "Let me try this with GPT-2's approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "c94c332a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-2's BPE tokenizer has a vocabulary of 50,257 tokens\n",
    "# I'll use 256-dimensional embeddings (matching GPT-2 small)\n",
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5dd69f",
   "metadata": {},
   "source": [
    "- I'll test this with a batch from my dataloader\n",
    "- With batch_size=8 and max_length=4, I expect shape: (8, 4, 256)\n",
    "- That's: 8 examples, 4 tokens each, 256-dimensional embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32d474a",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=8, max_length=max_length,\n",
    "    stride=max_length, shuffle=False\n",
    ")\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd67d1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Token IDs:\\n\", inputs)\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f295176",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(token_embeddings.shape)\n",
    "\n",
    "# uncomment & execute the following line to see how the embeddings look like\n",
    "print(token_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9407940a",
   "metadata": {},
   "source": [
    "- Now I need positional embeddings\n",
    "- GPT-2 uses **absolute position embeddings** - a learnable embedding for each position\n",
    "- I'll create another embedding layer, one for each position (0, 1, 2, 3 in my case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f212951",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "\n",
    "# I can peek at the position embedding weights if curious:\n",
    "print(pos_embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b8bdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create position embeddings for positions 0, 1, 2, 3\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
    "print(pos_embeddings.shape)\n",
    "\n",
    "# I can inspect the actual embeddings if needed:\n",
    "print(pos_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee897f7",
   "metadata": {},
   "source": [
    "- The final step: add token embeddings + positional embeddings\n",
    "- This gives the LLM both \"what is the token\" and \"where is it located\" information\n",
    "- I'm learning that this is the actual input the LLM will process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a09bdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(input_embeddings.shape)\n",
    "\n",
    "# I can inspect the final combined embeddings:\n",
    "print(input_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12aa4970",
   "metadata": {},
   "source": [
    "**My mental model so far:**\n",
    "\n",
    "The input processing pipeline I've built:\n",
    "1. **Raw text** → (tokenization) → **Token IDs** \n",
    "2. **Token IDs** → (embedding layer) → **Token embeddings** (what the token is)\n",
    "3. **Position indices** → (position embedding layer) → **Position embeddings** (where the token is)\n",
    "4. **Token embeddings + Position embeddings** → **Input embeddings** (ready for LLM)\n",
    "\n",
    "This is the foundation I'll build on in Chapter 3 when I learn about attention mechanisms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a123ce2",
   "metadata": {},
   "source": [
    "## My Chapter 2 Takeaways\n",
    "\n",
    "**What I learned:**\n",
    "- Tokenization: from simple regex splitting to BPE (byte-pair encoding)\n",
    "- Why GPT-2 uses BPE: handles unknown words by breaking them into subword units\n",
    "- Special tokens: `<|endoftext|>` for boundaries, `<|unk|>` for unknown words\n",
    "- Sliding window approach for creating training data from long text\n",
    "- Token embeddings: convert discrete IDs to continuous vectors\n",
    "- Positional embeddings: encode position information\n",
    "- Final input = token embeddings + positional embeddings\n",
    "\n",
    "**What I noticed:**\n",
    "- Stride affects overlap between training examples (more overlap → potential overfitting)\n",
    "- Embedding layers are essentially efficient lookup tables\n",
    "- Without positional embeddings, \"cat chased mouse\" looks identical to \"mouse chased cat\"\n",
    "\n",
    "**I still need to practice:**\n",
    "- Understanding the math behind why stride affects dataset size\n",
    "- Building intuition for embedding dimensions\n",
    "- Exploring the relationship between embeddings and one-hot encoding (bonus notebook)\n",
    "\n",
    "**Next steps:**\n",
    "- Chapter 3: Understanding attention mechanisms\n",
    "- Revisit BPE implementation in [../02_bonus_bytepair-encoder](../02_bonus_bytepair-encoder)\n",
    "- Study embedding vs matrix multiplication in [../03_bonus_embedding-vs-matmul](../03_bonus_embedding-vs-matmul)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms-from-scratch-practice (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
