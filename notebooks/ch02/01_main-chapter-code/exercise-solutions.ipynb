{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73a8c949",
   "metadata": {},
   "source": [
    "# My Exercise Solutions: Chapter 2 (Working with Text Data)\n",
    "\n",
    "**Date**: February 3, 2026\n",
    "\n",
    "**My goal**: I want to practice the Chapter 2 exercises in my own words and code to strengthen intuition about tokenization, vocab building, and data preparation.\n",
    "\n",
    "**Zero-copy note**: This notebook is my personal synthesis. I am not copying the source-material solutions.\n",
    "\n",
    "**Attribution**: Concepts are based on *Build a Large Language Model From Scratch* by Sebastian Raschka.\n",
    "\n",
    "**Scope note**: This notebook currently covers Exercises 2.1â€“2.2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8881b5",
   "metadata": {},
   "source": [
    "## Environment Check\n",
    "I want to record package versions for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875bc8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "print(\"torch version:\", version(\"torch\"))\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e8defe",
   "metadata": {},
   "source": [
    "## Reproducibility\n",
    "I set seeds so I can reproduce results later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaff037c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb9e6f2",
   "metadata": {},
   "source": [
    "## Exercise 2.1\n",
    "**Prompt (my words)**: Explore how the GPT-2 BPE tokenizer splits the string \"Akwirw ier\" and interpret the token IDs and decoded pieces.\n",
    "\n",
    "**My approach**: Use `tiktoken` to encode the string, print the token IDs, decode each token, and probe a few substrings to see where merges happen.\n",
    "\n",
    "**Solution**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f51e476",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "text = \"Akwirw ier\"\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "encoded = tokenizer.encode(text)\n",
    "\n",
    "print(\"Text:\", text)\n",
    "print(\"Token IDs:\", encoded)\n",
    "print(\"Decoded pieces:\")\n",
    "for token_id in encoded:\n",
    "    print(f\"  {token_id} -> {tokenizer.decode([token_id])}\")\n",
    "\n",
    "print(\"Reconstructed:\", tokenizer.decode(encoded))\n",
    "\n",
    "probe_strings = [\"Ak\", \"w\", \"ir\", \" \", \"ier\"]\n",
    "print(\"\\nSubstring probes:\")\n",
    "for snippet in probe_strings:\n",
    "    print(f\"  {snippet!r} -> {tokenizer.encode(snippet)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062ff858",
   "metadata": {},
   "source": [
    "## Exercise 2.2\n",
    "**Prompt (my words)**: Build a minimal GPT-style dataset and dataloader with a sliding window, then inspect a few batches using small `max_length` and `stride` values.\n",
    "\n",
    "**My approach**: Rebuild a simple `Dataset` class, load `the-verdict.txt`, create dataloaders with different window sizes, and print the first batch.\n",
    "\n",
    "**Solution**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35884cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class GPTDatasetMini(Dataset):\n",
    "    def __init__(self, text: str, tokenizer, max_length: int, stride: int) -> None:\n",
    "        token_ids = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        for start in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[start : start + max_length]\n",
    "            target_chunk = token_ids[start + 1 : start + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk, dtype=torch.long))\n",
    "            self.target_ids.append(torch.tensor(target_chunk, dtype=torch.long))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "\n",
    "def make_loader(text: str, max_length: int, stride: int, batch_size: int = 4):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDatasetMini(text, tokenizer, max_length=max_length, stride=stride)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as file_handle:\n",
    "    raw_text = file_handle.read()\n",
    "\n",
    "print(\"Example 1: max_length=2, stride=2\")\n",
    "loader_small = make_loader(raw_text, max_length=2, stride=2, batch_size=4)\n",
    "for x_batch, y_batch in loader_small:\n",
    "    print(\"x:\", x_batch)\n",
    "    print(\"y:\", y_batch)\n",
    "    break\n",
    "\n",
    "print(\"\\nExample 2: max_length=8, stride=2\")\n",
    "loader_medium = make_loader(raw_text, max_length=8, stride=2, batch_size=4)\n",
    "for x_batch, y_batch in loader_medium:\n",
    "    print(\"x:\", x_batch)\n",
    "    print(\"y:\", y_batch)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms-from-scratch-practice (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
