# Chapter 3: Cording Attention Mechanisms

**Phase 3: Transformer Architecture**

## Focus
This chapter focuses on the core mechanism that makes Transformers work: **Self-Attention**.

## Learning Goals
- Understand the difference between Self-Attention and Causal Self-Attention.
- Implement the "Scale Dot-Product Attention" mechanism from scratch.
- Build Multi-Head Attention modules.
- Visualize attention weights to understand what the model is "looking at".

## Notebooks
- `01_attention_mechanisms.ipynb`: Building self-attention step-by-step.
